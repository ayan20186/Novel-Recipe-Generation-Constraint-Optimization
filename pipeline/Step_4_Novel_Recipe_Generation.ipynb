{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Mounting Google Drive for importing the Data Files which will be used in the Tokenization**"],"metadata":{"id":"kTts_zswOGQ7"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"IGpbSsbJOUu5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701281494908,"user_tz":-330,"elapsed":29849,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}},"outputId":"7caf02a8-87de-4aba-f98a-980e97371efd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"IrDC5mrPMPs9"},"source":["**Downloading, Installing & Importing Required Libraries**"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HC_GK_m7zkQa","executionInfo":{"status":"ok","timestamp":1701281508745,"user_tz":-330,"elapsed":7777,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}},"outputId":"92e41906-89a1-4995-b773-41d642f8e4ca"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}]},{"cell_type":"code","source":["import re\n","import os\n","import math\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import trange\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","from transformers import  AutoTokenizer,AutoModelWithLMHead"],"metadata":{"id":"B0ulhDaBiuND","executionInfo":{"status":"ok","timestamp":1701281513422,"user_tz":-330,"elapsed":4680,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["**Importing Recipe Ingredient Tables**"],"metadata":{"id":"DK9oArdli0qo"}},{"cell_type":"code","source":["table = pd.read_csv(\"/content/drive/MyDrive/BTP_Dev/Dataset/RecipeDB_ingredient_phrase.csv\")"],"metadata":{"id":"8NCXNw_piyFe","executionInfo":{"status":"ok","timestamp":1701281516727,"user_tz":-330,"elapsed":3307,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["**Fetching the \"recipe_no\" and \"ingredient\" columns**"],"metadata":{"id":"JhvwrbFIi65R"}},{"cell_type":"code","source":["recipe_ingredient_table = table[['recipe_no', 'ingredient']].copy()"],"metadata":{"id":"44NoK_pvi8TM","executionInfo":{"status":"ok","timestamp":1701281516728,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["**Observation: Same ingredient is used more than once in the same recipe, for example \"water\" is used more than once in the recipe \"2610.0\"**"],"metadata":{"id":"sYueXMRQi_pV"}},{"cell_type":"markdown","source":["**Removing Duplicate rows**"],"metadata":{"id":"fQez_Lv3jC1V"}},{"cell_type":"code","source":["recipe_ingredient_table_unique = recipe_ingredient_table.drop_duplicates(keep = 'first')\n","recipe_ingredient_table_unique = recipe_ingredient_table_unique[~recipe_ingredient_table_unique['ingredient'].isna()]"],"metadata":{"id":"Axb8BVHEjFEM","executionInfo":{"status":"ok","timestamp":1701281517210,"user_tz":-330,"elapsed":485,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["**Table that maps Recipe number to its ingredients result is a Dictionary that maps Recipe number to its ingredients list**"],"metadata":{"id":"vicqPhWyjMjO"}},{"cell_type":"code","source":["result=recipe_ingredient_table_unique.groupby('recipe_no')['ingredient'].apply(list).to_dict()\n","keys = list(result.keys())\n","values = list(result.values())\n","recipe_size =[ len(listElem) for listElem in values]"],"metadata":{"id":"aB8xPmFojRt3","executionInfo":{"status":"ok","timestamp":1701281522311,"user_tz":-330,"elapsed":5103,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["**final_df1 contains recipe_no, ingredients and recipe_size**"],"metadata":{"id":"Ax-eDZRcjdHn"}},{"cell_type":"code","source":["df1 = pd.DataFrame(list(zip(keys,values,recipe_size)),columns=['recipe_no','ingredients','recipe_size'])\n","final_df1 = df1.sort_values(by=['recipe_size'])\n","recipe_size_1 = final_df1.loc[final_df1['recipe_size'] == 1]\n","recipe_id_size_one_list = recipe_size_1['recipe_no'].tolist()\n","recipe_size_1_cooking_procedure = table[table['recipe_no'].isin(recipe_id_size_one_list)]"],"metadata":{"id":"AH67SfIyjiMJ","executionInfo":{"status":"ok","timestamp":1701281523044,"user_tz":-330,"elapsed":736,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["**Removing recipes from the \"recipe_ingredient_table_unique table\" with size equal to 1**"],"metadata":{"id":"ShGR_coZjjGI"}},{"cell_type":"code","source":["recipe_ingredient_table_unique = recipe_ingredient_table_unique[~recipe_ingredient_table_unique['recipe_no'].isin(recipe_id_size_one_list)]"],"metadata":{"id":"0pclkogsjnlP","executionInfo":{"status":"ok","timestamp":1701281523045,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["**Finding count of each ingredient across the recipes**"],"metadata":{"id":"_whd8h4bjqUp"}},{"cell_type":"code","source":["df_count = recipe_ingredient_table_unique['ingredient'].value_counts()\n","recipe_ingredient_table_count = pd.DataFrame({'ingredient': df_count.index, 'Recipe_Count':df_count.values})"],"metadata":{"id":"wkjEOaO0jpqh","executionInfo":{"status":"ok","timestamp":1701281523648,"user_tz":-330,"elapsed":605,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["**Evaluating the PMF(Probability Mass Function) and CDF(Cumulative Distribution Function) values for each ingredient**"],"metadata":{"id":"Dsqdlzymjvhl"}},{"cell_type":"code","source":["ingredients_count = recipe_ingredient_table_count.shape[0]                             ## ingredients_count is the total number of unique ingredients across all the recipes\n","recipe_count_list = recipe_ingredient_table_count['Recipe_Count'].tolist()             ## recipe_count_list contains the list of recipe_count for each ingredient\n","recipe_count_list_unique = recipe_ingredient_table_count['Recipe_Count'].unique()      ## recipe_count_list_unique contains the unique values of recipe_counts\n","\n","pmf_list_unique = []                                                                   ## pmf_list_unique contains the pmf values corresponding to each recipe count\n","for item in recipe_count_list_unique:\n","    a = recipe_count_list.count(item)\n","    # print(a)\n","    pmf = a / ingredients_count\n","    pmf_list_unique.append(pmf)\n","\n","cdf = 0                                                                                ## cdf_list_unique contains the cdf values corresponding to each recipe count\n","cdf_list_unique = []\n","for pmf in pmf_list_unique:\n","    cdf = cdf + pmf\n","    cdf_list_unique.append(cdf)\n","\n","data = {'Recipe_Count': recipe_count_list_unique ,'Pmf': pmf_list_unique, 'Cdf': cdf_list_unique}\n","df = pd.DataFrame(data)\n","\n","df1 = pd.merge(recipe_ingredient_table_count, df, how='inner', on = 'Recipe_Count')"],"metadata":{"id":"nCqodAnwjziG","executionInfo":{"status":"ok","timestamp":1701281525564,"user_tz":-330,"elapsed":1919,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["**Creating Input Function that will perform the following tasks:**"],"metadata":{"id":"UJWEuVxoj9Y7"}},{"cell_type":"markdown","source":["**1. Taking random n(number of ingredients to select) and fetching same number of ingredients based on random cdf values selected.**"],"metadata":{"id":"Gk_pqWCekIOe"}},{"cell_type":"markdown","source":["**2. In case the randomly selected cdf value belongs to more than one ingredients, then we select any one of them randomly.**"],"metadata":{"id":"tNMhYtVXkNLv"}},{"cell_type":"markdown","source":["**3. Removing Duplicate Ingredients.**"],"metadata":{"id":"QdrfUKGJkYP-"}},{"cell_type":"markdown","source":["**4. Coverting list to ingredients to single string of the form which is compatible with the out GPT2 model.**"],"metadata":{"id":"OdC73Nefkf9w"}},{"cell_type":"code","source":["def takeRandomInput():\n","  cdfValues=df['Cdf'].tolist()\n","  ingredientsChoices=[2,3,4,5,6,7,8]\n","  randomNumberOfIngredients=random.choice(ingredientsChoices)\n","  inputIngredientsList=list()\n","  for i in range(0,randomNumberOfIngredients):\n","    currentRandomCdf=random.choice(cdfValues)\n","    currentCdfIngredeintsList=list()\n","    for ind in df1.index:\n","      if(df1['Cdf'][ind]==currentRandomCdf):\n","        currentCdfIngredeintsList.append(df1['ingredient'][ind])\n","    inputIngredientsList.append(random.choice(currentCdfIngredeintsList))\n","\n","  res = []\n","  for i1 in inputIngredientsList:\n","    if i1 not in res:\n","      res.append(i1)\n","\n","  inputIngredientsString=str()\n","  for eachIngredeint in res:\n","    inputIngredientsString=str(eachIngredeint)+str(\",\")+inputIngredientsString\n","  inputIngredientsString=inputIngredientsString[0:len(inputIngredientsString)-1]\n","  inputIngredientsString=inputIngredientsString+str(\";\")\n","  return inputIngredientsString"],"metadata":{"id":"SoqdLGYCj7fN","executionInfo":{"status":"ok","timestamp":1701281525564,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["takeRandomInput()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"rxN0YzgfHJyU","executionInfo":{"status":"ok","timestamp":1701281526893,"user_tz":-330,"elapsed":1331,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}},"outputId":"e6864383-ce6c-4f0e-b7d6-d2a14a1a1a6a"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'orange bell pepper,courgette,fish,raspberry,peach;'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["**Building Model Pre-Requisites**"],"metadata":{"id":"ImOxwhF4kmBg"}},{"cell_type":"code","source":["def set_seed(seed):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)"],"metadata":{"id":"of2K2DmTkp_b","executionInfo":{"status":"ok","timestamp":1701281526893,"user_tz":-330,"elapsed":5,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n","    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n","        Args:\n","            logits: logits distribution shape (vocabulary size)\n","            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n","            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n","                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n","        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n","    \"\"\"\n","    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n","    top_k = min(top_k, logits.size(-1))  # Safety check\n","    if top_k > 0:\n","        # Remove all tokens with a probability less than the last token of the top-k\n","        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n","        logits[indices_to_remove] = filter_value\n","    if top_p > 0.0:\n","        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","\n","        # Remove tokens with cumulative probability above the threshold\n","        sorted_indices_to_remove = cumulative_probs > top_p\n","        # Shift the indices to the right to keep also the first token above the threshold\n","        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","        sorted_indices_to_remove[..., 0] = 0\n","        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","        logits[indices_to_remove] = filter_value\n","    return logits"],"metadata":{"id":"tKnVNW7Jktrf","executionInfo":{"status":"ok","timestamp":1701281526893,"user_tz":-330,"elapsed":4,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def sample_sequence(model, length, context, tokenizer, num_samples=1, temperature=1, top_k=0, top_p=0.0, device = 'gpu'):\n","    end_token = tokenizer.convert_tokens_to_ids([\"<END_RECIPE>\"])[0]\n","    context = torch.tensor(context, dtype=torch.long, device=device)\n","    context = context.unsqueeze(0).repeat(num_samples, 1)\n","    generated = context\n","    with torch.no_grad():\n","        for _ in trange(length):\n","            inputs = {'input_ids': generated}\n","            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n","            next_token_logits = outputs[0][0, -1, :] / temperature\n","            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n","            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n","            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n","            if next_token.item() == end_token:\n","                print('breaking----->>')\n","                break\n","    return generated"],"metadata":{"id":"vVexaBHslANs","executionInfo":{"status":"ok","timestamp":1701281526893,"user_tz":-330,"elapsed":4,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["set_seed(20)"],"metadata":{"id":"uT2XUY28lCA4","executionInfo":{"status":"ok","timestamp":1701281526893,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["**Defining the Method that will generate the Novel recipe by providing the list of Input Ingredients to Trained GPT2 Model**"],"metadata":{"id":"Vuojh2SllI6z"}},{"cell_type":"code","source":["def startRatatouileModel(ingredientsList, cuisine):\n","  #Prepares model and provides the above random generated ingredients to Ratatouile model\n","  MODEL_CLASSES = {\n","    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer),\n","  }\n","  MODEL_CLASSES1 = {\n","    'gpt2': (AutoModelWithLMHead, AutoTokenizer),\n","  }\n","  model_class, tokenizer_class = MODEL_CLASSES['gpt2']\n","  tokenizer = tokenizer_class.from_pretrained('/content/drive/My Drive/BTP_Dev/output_top5_token')\n","  model = model_class.from_pretrained('/content/drive/My Drive/BTP_Dev/output_top5_token')\n","  model.to(torch.device(\"cuda\" ))\n","  model.eval()\n","\n","  raw_text=ingredientsList\n","\n","  prepared_input = f'<CUISINE_{cuisine.upper()}> <RECIPE_START><INPUT_START> ' + ingredientsList.replace(',', ' <NEXT_INPUT> ').replace(';', ' <INPUT_END>')\n","  context_tokens = tokenizer.encode(prepared_input)\n","\n","  out = sample_sequence(\n","    model=model,\n","    context=context_tokens,\n","    tokenizer=tokenizer,\n","    length=768,\n","    temperature=1,\n","    top_k=30,\n","    top_p=1,\n","    device=torch.device(\"cuda\")\n","  )\n","  out = out[0, len(context_tokens):].tolist()\n","  text = tokenizer.decode(out, clean_up_tokenization_spaces=True)\n","  print(tokenizer.decode)\n","  if \"<RECIPE_END>\" not in text:\n","    print(text)\n","    print(\"Failed to generate, recipe's too long\")\n","  return text, prepared_input"],"metadata":{"id":"DxsdBb2UlE7j","executionInfo":{"status":"ok","timestamp":1701281526893,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["**Defining the Final Dataframe that will contain the generated Novel Recipes**"],"metadata":{"id":"a0-8-L_XlvL_"}},{"cell_type":"markdown","source":["**Defining the variable that will define how many novel recipes we want to generate using the loop, By default, we are setting it to 10k, change according to your need.**"],"metadata":{"id":"nnkQThW_l8RA"}},{"cell_type":"code","source":["def generate_and_save_recipes(number_of_recipes, cuisine, output_path):\n","    # Initialize an empty dataframe\n","    novelRecipesDataframe = pd.DataFrame(columns=['Random Ingredients', 'Recipe Title', 'Ingredient Phrases', 'Recipe Instructions'])\n","\n","    for i in range(number_of_recipes):\n","        randomIngredients = takeRandomInput()\n","\n","        novelRecipeGenerated, user_input = startRatatouileModel(randomIngredients, cuisine)\n","        # Process the novel recipe as per your specified logic\n","        generated_recipe = process_recipe(novelRecipeGenerated)\n","\n","        # Extracting parts of the generated recipe\n","        rnidx = generated_recipe.find(\"Name:- ##\\n\")\n","        igidx = generated_recipe.find(\"dients ##\\n\")\n","        instnidx = generated_recipe.find(\"uctions ##\\n\")\n","        lastidx = generated_recipe.find(\"\\n\\n\\n\\n\\n\\n\")\n","\n","        resname = generated_recipe[rnidx + 11:igidx-12]\n","        ings = generated_recipe[igidx+10:instnidx-19].lower()\n","        instn = format_instructions(generated_recipe[instnidx+11:lastidx])\n","\n","        # Adding the recipe to the dataframe\n","        df2 = {'Random Ingredients': randomIngredients, 'Recipe Title': resname, 'Ingredient Phrases': ings, 'Recipe Instructions': instn}\n","        novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n","\n","    # Save the dataframe to a CSV file\n","    novelRecipesDataframe.to_csv(output_path, index=False)\n","\n","def process_recipe(recipe):\n","    return str(recipe.replace('<RECIPE_START> <INPUT_START>', '## User inputs ##\\n    -').replace('<NEXT_INPUT>', '\\n    -').replace('<INPUT_END>', '\\n------------------------\\n\\n')\\\n","                      .replace('<TITLE_START>', '## Recipe Name:- ##\\n').replace('<TITLE_END>', '\\n')\\\n","                      .replace('<INGR_START>', '\\n## Ingredients ##\\n').replace('<NEXT_INGR>', '|').replace('<INGR_END>', '\\n\\n')\\\n","                      .replace('<INSTR_START>', '## Cooking instructions ##\\n').replace('.','.\\n    -').replace(' <NEXT_INSTR>', '. ').replace(' <INSTR_END>', '. ')\\\n","                      .replace(' <RECIPE_END>', '\\n\\n\\n\\nVoila Enjoy your recipe :)\\n\\n\\n\\n\\n -----------\\n'))\n","\n","def format_instructions(instructions):\n","    its = instructions.split(' ')\n","    for i in range(len(its)):\n","        if i < len(its) - 1 and its[i].isnumeric() and its[i+1].isnumeric():\n","            its.insert(i+1, \"-\")\n","    return \" \".join(its)\n","\n","# List of cuisines\n","cuisines = ['ITALIAN', 'MEXICAN', 'INDIAN SUBCONTINENT', 'SOUTH AMERICAN', 'CANADIAN']\n","number_of_recipes = 10\n","base_path = '/content/drive/My Drive/BTP_Dev/Final_DF/'\n","\n","for cuisine in cuisines:\n","    output_path = f'{base_path}NovelRecipesGenerated_{cuisine}.csv'\n","    generate_and_save_recipes(number_of_recipes, cuisine, output_path)\n","\n","print(\"Recipes for all cuisines generated and saved successfully.\")\n"],"metadata":{"id":"HXClGVFDl8p7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701284887686,"user_tz":-330,"elapsed":1786816,"user":{"displayName":"Dev Thakkar","userId":"02582420650664740577"}},"outputId":"dd1706f7-f5cb-4f14-e85f-c4d806584810"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.90it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:29<00:00, 26.07it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:30<00:00, 25.10it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.02it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.56it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:33<00:00, 22.62it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.58it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.31it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.58it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.74it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.64it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.91it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:33<00:00, 23.23it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.40it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.41it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.53it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.63it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:33<00:00, 22.90it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:33<00:00, 23.10it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.40it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.57it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.78it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.53it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:33<00:00, 22.80it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.39it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.37it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.74it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.00it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.93it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.50it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:33<00:00, 22.91it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.96it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:33<00:00, 23.24it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:33<00:00, 23.12it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.36it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:33<00:00, 23.17it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.49it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.66it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.92it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.39it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.05it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.64it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.44it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:33<00:00, 22.67it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.02it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.68it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:32<00:00, 23.47it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.10it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.09it/s]\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","100%|██████████| 768/768 [00:31<00:00, 24.15it/s]"]},{"output_type":"stream","name":"stdout","text":["<bound method PreTrainedTokenizerBase.decode of GPT2Tokenizer(name_or_path='/content/drive/My Drive/BTP_Dev/output_top5_token', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<CUISINE_MEXICAN>', '<INGR_END>', '<TITLE_END>', '<NEXT_INSTR>', '<NEXT_INGR>', '<INPUT_START>', '<CUISINE_INDIAN SUBCONTINENT>', '<RECIPE_START>', '<CUISINE_CANADIAN>', '<INSTR_START>', '<INSTR_END>', '<RECIPE_END>', '<CUISINE_ITALIAN>', '<CUISINE_SOUTH AMERICAN>', '<TITLE_START>', '<INGR_START>', '<INPUT_END>', '<NEXT_INPUT>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t50257: AddedToken(\"<CUISINE_MEXICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50258: AddedToken(\"<INGR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50259: AddedToken(\"<TITLE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50260: AddedToken(\"<NEXT_INSTR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50261: AddedToken(\"<NEXT_INGR>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50262: AddedToken(\"<INPUT_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50263: AddedToken(\"<CUISINE_INDIAN SUBCONTINENT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50264: AddedToken(\"<RECIPE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50265: AddedToken(\"<CUISINE_CANADIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50266: AddedToken(\"<INSTR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50267: AddedToken(\"<INSTR_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50268: AddedToken(\"<RECIPE_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50269: AddedToken(\"<CUISINE_ITALIAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50270: AddedToken(\"<CUISINE_SOUTH AMERICAN>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50271: AddedToken(\"<TITLE_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50272: AddedToken(\"<INGR_START>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50273: AddedToken(\"<INPUT_END>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t50274: AddedToken(\"<NEXT_INPUT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}>\n","Recipes for all cuisines generated and saved successfully.\n"]},{"output_type":"stream","name":"stderr","text":["\n","<ipython-input-21-11cb0f9c4b37>:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  novelRecipesDataframe = novelRecipesDataframe.append(df2, ignore_index=True)\n"]}]},{"cell_type":"markdown","source":["**Saving the Final Dataframe that contains all the Novel Recipes Generated**"],"metadata":{"id":"pzvv2ZDGmlWR"}}]}